{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fd88c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "171325bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a41ce9ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9338c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'a',\n",
       " 2: 'b',\n",
       " 3: 'c',\n",
       " 4: 'd',\n",
       " 5: 'e',\n",
       " 6: 'f',\n",
       " 7: 'g',\n",
       " 8: 'h',\n",
       " 9: 'i',\n",
       " 10: 'j',\n",
       " 11: 'k',\n",
       " 12: 'l',\n",
       " 13: 'm',\n",
       " 14: 'n',\n",
       " 15: 'o',\n",
       " 16: 'p',\n",
       " 17: 'q',\n",
       " 18: 'r',\n",
       " 19: 's',\n",
       " 20: 't',\n",
       " 21: 'u',\n",
       " 22: 'v',\n",
       " 23: 'w',\n",
       " 24: 'x',\n",
       " 25: 'y',\n",
       " 26: 'z',\n",
       " 0: '.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = { s: i + 1 for i, s in enumerate(chars) }\n",
    "stoi['.'] = 0\n",
    "itos = { i: s for s, i in stoi.items() }\n",
    "itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a0e27e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 3\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    \n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "            \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19d4ec19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a12d5771",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(words)\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte,  Yte  = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a53019bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n"
     ]
    }
   ],
   "source": [
    "# Our previous MLP implementation\n",
    "vocab_size = len(stoi)\n",
    "n_embed = 10 # the dimensions for character embedding in a vector\n",
    "n_hidden = 200 # number of neurons in the hidden layer\n",
    "\n",
    "g  = torch.Generator().manual_seed(2147483647)\n",
    "C  = torch.randn((vocab_size, n_embed),    generator = g) # our initial lookup table (an n_embed dimensional embedding)\n",
    "W1 = torch.randn((n_embed * block_size, n_hidden),      generator = g) * 0.2 # our hidden layer matrix\n",
    "b1 = torch.randn(n_hidden,                 generator = g) * 0.01 # our biases \n",
    "W2 = torch.randn((n_hidden, vocab_size),   generator = g) * 0.01 # our output layer\n",
    "b2 = torch.randn(vocab_size,               generator = g) * 0\n",
    "\n",
    "# batch normalization\n",
    "bngain = torch.ones((1, n_hidden))\n",
    "bnbias = torch.zeros((1, n_hidden))\n",
    "bnrunning_mean = torch.zeros((1, n_hidden))\n",
    "bnrunning_std = torch.ones((1, n_hidden))\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e13024c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 200])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpreact.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c067bbd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 200])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpreact.mean(0, keepdim=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fccc74e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 200])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpreact.std(0, keepdim=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "09ad49ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.3147\n",
      "  10000/ 200000: 2.1341\n",
      "  20000/ 200000: 2.3173\n",
      "  30000/ 200000: 2.4115\n",
      "  40000/ 200000: 2.0124\n",
      "  50000/ 200000: 2.3003\n",
      "  60000/ 200000: 2.4708\n",
      "  70000/ 200000: 2.1208\n",
      "  80000/ 200000: 2.3562\n",
      "  90000/ 200000: 2.1095\n",
      " 100000/ 200000: 1.9329\n",
      " 110000/ 200000: 2.3919\n",
      " 120000/ 200000: 1.9795\n",
      " 130000/ 200000: 2.4764\n",
      " 140000/ 200000: 2.3703\n",
      " 150000/ 200000: 2.3098\n",
      " 160000/ 200000: 2.0003\n",
      " 170000/ 200000: 1.8345\n",
      " 180000/ 200000: 2.0355\n",
      " 190000/ 200000: 1.8853\n"
     ]
    }
   ],
   "source": [
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "    \n",
    "    # create our mini-batch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X, Y\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[Xb]                         # embed the characters into vectors\n",
    "    embcat = emb.view(emb.shape[0], -1) # concat the vectors\n",
    "    hpreact = embcat @ W1 + b1          # hidden layer pre-activation\n",
    "    \n",
    "    # batch normalization\n",
    "    # each neuron will be normalized to a gaussain distribution\n",
    "    hpreact = bngain * (hpreact - hpreact.mean(0, keepdim=True)) / hpreact.std(0, keepdim=True) + bnbias\n",
    "    \n",
    "    # update our running mean\n",
    "    with torch.no_grad():\n",
    "        bnrunning_mean = 0.999 * bnrunning_mean + 0.001 * hpreact.mean(0, keepdim=True)\n",
    "        bnrunning_std  = 0.999 * bnrunning_std + 0.001 * hpreact.std(0, keepdim=True)\n",
    "    \n",
    "    h = torch.tanh(hpreact)\n",
    "    \n",
    "    logits = h @ W2 + b2                # output layer\n",
    "    loss = F.cross_entropy(logits, Yb)  # loss function\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "        \n",
    "    # track stats\n",
    "    if i % 10000 == 0:\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aae27a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1225fffd0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(h.abs() > 0.99, cmap=\"gray\", interpolation=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6eb92b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.9265358448028564\n",
      "val 2.9504737854003906\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad() # avoid gradient tracking here (no back-propagation)\n",
    "def split_loss(split):\n",
    "    x,y = {\n",
    "        'train': (Xtr, Ytr),\n",
    "        'val':   (Xdev, Ydev),\n",
    "        'test':  (Xte, Yte)\n",
    "    }[split]\n",
    "    \n",
    "    emb = C[x]\n",
    "    embcat = emb.view(emb.shape[0], -1) # concat the vectors\n",
    "    hpreact = embcat @ W1 + b1          # hidden layer pre-activation\n",
    "    \n",
    "    # batch normalization, train and scale\n",
    "    #hpreact = bngain * (hpreact - hpreact.mean(0, keepdim=True)) / hpreact.std(0, keepdim=True) + bnbias\n",
    "    hpreact = bngain * (hpreact - bnrunning_mean) / bnrunning_std + bnbias\n",
    "    \n",
    "    h = torch.tanh(hpreact)\n",
    "    \n",
    "    logits = h @ W2 + b2                # output layer\n",
    "    loss = F.cross_entropy(logits, y)  # loss function\n",
    "    print(split, loss.item())\n",
    "    \n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3201361e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2ec62883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carmahzauri.\n",
      "jai.\n",
      "kimri.\n",
      "reh.\n",
      "casparsha.\n",
      "jazhith.\n",
      "deliah.\n",
      "jareei.\n",
      "nellara.\n",
      "chaily.\n",
      "kaleigh.\n",
      "ham.\n",
      "jorn.\n",
      "quint.\n",
      "salin.\n",
      "alianni.\n",
      "waythoniearyn.\n",
      "kai.\n",
      "euliyuan.\n",
      "eddeli.\n"
     ]
    }
   ],
   "source": [
    "# sample from our model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size\n",
    "    while True:\n",
    "        \n",
    "        emb = C[torch.tensor([context])]\n",
    "        h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        \n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        \n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        \n",
    "        if ix == 0:\n",
    "            break\n",
    "            \n",
    "    print(\"\".join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "993a948a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2638, 0.0734, 0.2708, 0.3919]), tensor(1.3063))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = torch.randn(4)\n",
    "probs = torch.softmax(logits, dim=0)\n",
    "loss = -probs[2].log()\n",
    "probs, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0122ed72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is problematic... \n",
    "# when we backpropagate, there is no multiplying the gradient\n",
    "plt.hist(h.view(-1).tolist()); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3784414b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0025) tensor(1.0115)\n",
      "tensor(-0.0018) tensor(1.0271)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1000, 10)\n",
    "w = torch.randn(10, 200) / 10**0.5\n",
    "y = x @ w\n",
    "print(x.mean(), x.std())\n",
    "print(y.mean(), y.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5bc303aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0096)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.randn(10000)).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "377270be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2002)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.randn(10000) * 0.2).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c40d6bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3042903097250923"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fanin = n_embed * block_size\n",
    "gain = (5/3) / (fanin ** 0.5) # gain for tanh * sqrt of fanin\n",
    "gain # what we want the standard deviation to be!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae134e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' SUMMARY '''\n",
    "\n",
    "class Linear:\n",
    "    \n",
    "    def __init__(self, fan_in, fan_out, bias=True):\n",
    "        self.weight = torch.randn((fan_in, fan_out), generator=g) / fan_in**0.5\n",
    "        self.bias = torch.zeros((fan_out)) if bias else None\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.weight\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.weight] + ([] if self.bias is None esle [self.bias])\n",
    "    \n",
    "class BatchNorm1d:\n",
    "    \n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "        self.gain = torch.ones(dim)\n",
    "        self.bias = torch.zeros(dim)\n",
    "        self.running_mean = torch.zeros(dim)\n",
    "        self.running_var = torch.ones(dim)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        if self.training:\n",
    "            xmean = x.mean(0, keepdim=True)\n",
    "            xvar  = x.var(0, keepdim=True)\n",
    "        else:\n",
    "            xmean = self.running_mean\n",
    "            xvar  = self.running_var\n",
    "            \n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "        self.out = self.gain * xhat + self.bias\n",
    "        \n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                exst_weight = 1 - self.momentum\n",
    "                self.running_mean = (exst_weight * self.running_mean + self.momentum * xmean)\n",
    "                self.running_var  = (exst_weight * self.running_var + self.momentum * xvar)\n",
    "\n",
    "        return self.out\n",
    "        \n",
    "    def parameters(self):\n",
    "        return [self.gain, self.bias]\n",
    "    \n",
    "    \n",
    "class Tanh:\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "    \n",
    "    def parameters():\n",
    "        return []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
